{
    "cells": [
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "<img src=\"logo.png\" alt=\"Logo M\u00e1ster\" align=\"left\" height=\"62\" width=\"300\">\n\n<br><br><br><br>\n\n# Programaci\u00f3n de NaiveBayes con Spark\n\n\n### Luis de la Ossa\n\n\nEscuela Superior de Ingenier\u00eda Inform\u00e1tica de Albacete\n\n\n*Universidad de Castilla-La Mancha*\n"
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "---\nEl objetivo de este tutorial es hacer una toma de contacto con la programaci\u00f3n en Spark que, como se ha visto en clase, es funcional, y requiere un cambio de planteamiento con respecto a c\u00f3mo se programa en programaci\u00f3n imperativa. En concreto, implementaremos la versi\u00f3n m\u00e1s b\u00e1sica del algoritmo de clasificaci\u00f3n supervisada _Naive Bayes_ en el entorno _Spark_. "
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "---\n\nAntes de empezar, hemos de comprobar que el contexto de Spark est\u00e1 activo."
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "outputs": [
                {
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "<pyspark.context.SparkContext at 0x7f451055ee48>"
                    }, 
                    "execution_count": 1
                }
            ], 
            "source": "sc", 
            "execution_count": 1
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "---\n### Lectura de datos\n\nEn primer lugar, vamos a crear un _RDD_ denominado `dataRDD` con los datos. En primer lugar, hay que leer el archivo con `sc.textFile`. Esto genera un _RDD_ con l\u00edneas de texto. Sin embargo, cada l\u00ednea contiene una serie de valores separados por comas, por lo que hay que procesarla y transformarla en una lista de valores mediante la funci\u00f3n `split` del objeto `String`. \n\n__Nota__: Si se trabaja en modo local, hay que poner el camino absoluto a la localizaci\u00f3n del archivo."
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(sc)\n\n# @hidden_cell\n# This function is used to setup the access of Spark to your Object Storage. The definition contains your credentials.\n# You might want to remove those credentials before you share your notebook.\ndef set_hadoop_config_with_credentials_74af1899931d4f94bb91ee5a4f761112(name):\n    \"\"\"This function sets the Hadoop configuration so it is possible to\n    access data from Bluemix Object Storage using Spark\"\"\"\n\n    prefix = 'fs.swift.service.' + name\n    hconf = sc._jsc.hadoopConfiguration()\n    hconf.set(prefix + '.auth.url', 'https://identity.open.softlayer.com'+'/v3/auth/tokens')\n    hconf.set(prefix + '.auth.endpoint.prefix', 'endpoints')\n    hconf.set(prefix + '.tenant', 'e67e4544bd794bb7bf3a337a94365c28')\n    hconf.set(prefix + '.username', '1f432d6a92284e7382af2539f883b401')\n    hconf.set(prefix + '.password', 'iaYr2?EoV#1GUnM2')\n    hconf.setInt(prefix + '.http.port', 8080)\n    hconf.set(prefix + '.region', 'dallas')\n    hconf.setBoolean(prefix + '.public', False)\n\n# you can choose any name\nname = 'keystone'\nset_hadoop_config_with_credentials_74af1899931d4f94bb91ee5a4f761112(name)\n\n# Please read the documentation of PySpark to learn more about the possibilities to load data files.\n# PySpark documentation: https://spark.apache.org/docs/1.6.0/api/python/pyspark.sql.html#pyspark.sql.SQLContext\n# The SQLContext object is already initalized for you.\n# The following variable contains the path to your file on your Object Storage.\npath_1 = \"swift://PracticaSpark.\" + name + \"/coches.txt\"\n", 
            "execution_count": 1
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "dataRDD = sc.textFile(path_1).map(lambda linea: linea.split(','))", 
            "execution_count": 2
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "Ahora, muestra las cinco primeras l\u00edneas mediante la funci\u00f3n `take`."
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "outputs": [
                {
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "[['ALQUILER', 'VELOCIDAD', 'CARRETERA', 'VEHICULO'],\n ['Barato', 'Normal', 'Nacional', 'Berlina'],\n ['Barato', 'Normal', 'Nacional', 'Berlina'],\n ['Barato', 'Normal', 'Nacional', 'Berlina'],\n ['Caro', 'Normal', 'Nacional', 'Berlina']]"
                    }, 
                    "execution_count": 3
                }
            ], 
            "source": "dataRDD.take(5)", 
            "execution_count": 3
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "Para facilitar la lectura del c\u00f3digo posterior, vamos a extraer los nombres de las variables, y almacenarlos en la variable `features`. Tambi\u00e9n se obtendr\u00e1 el n\u00famero de variables, que ha de ser almacenado en la variable `n_feat`, y se almacenar\u00e1 el nombre de la clase en la variable `class_feature`."
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "features = dataRDD.take(1)[0]\nn_feat = len(features)\nclass_feature = features[-1]", 
            "execution_count": 4
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "Una vez le\u00eddos y almacenados los nombres de las variables, los eliminamos del RDD que contiene los datos. Para ello, utilizamos la funci\u00f3n `filter`. Tambi\u00e9n guardaremos el n\u00famero de instancias de la base de datos en la variable `n_inst`."
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "outputs": [
                {
                    "text": "16\n", 
                    "name": "stdout", 
                    "output_type": "stream"
                }
            ], 
            "source": "# COMPLETAR\n\ndataRDD = dataRDD.filter(lambda x: x!=features)\nn_inst = dataRDD.count()\nprint(n_inst)\n#16", 
            "execution_count": 5
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "---\n\n\n<br>\n### Naive Bayes: Aprendizaje del modelo\n\nEl clasificador Naive Bayes factoriza la distribuci\u00f3n de probabilidad conjunta (DPC) $P(X_1,\\dots,X_n,Y)$ como:\n\n$$\nP(x_1,\\dots,x_n,y) \\, = \\,  \\prod_{i=1}^{n}  P(x_i|y) \\cdot P(y)\n$$\n\nPor tanto, el aprendizaje del modelo solamente consiste en estimar $P(Y)$ y $P(X_i|Y)$ para cada variable $P(X_i)$.  \n"
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "---\n\n#### Definici\u00f3n de las variables\n\nEn primer lugar, hemos de obtener los valores que puede tomar cada una de las variables, ya que ser\u00e1n necesarios posteriormente. Hemos de pensar que la base de datos est\u00e1 almacenada en un RDD, y podr\u00eda tener millones de registros, y que obtener los valores de cada variable, a priori desconocidos, implica recorrer la base de datos. Para llevar a cabo este proceso, utilizamos la funci\u00f3n `distinct` de Spark.  \n\nLa lista de valores que puede tomar cada variable, la almacenaremos a su vez en otra lista denominada `feat_values`, y el n\u00famero de valores que puede tomar cada variable en la lista denominada `num_feat_values`."
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "outputs": [
                {
                    "text": "{'CARRETERA': ['Nacional', 'Autov\u00eda'], 'VELOCIDAD': ['Normal', 'Alta'], 'ALQUILER': ['Caro', 'Barato'], 'VEHICULO': ['Berlina', 'Deportivo']}\n{'CARRETERA': 2, 'VELOCIDAD': 2, 'ALQUILER': 2, 'VEHICULO': 2}\n", 
                    "name": "stdout", 
                    "output_type": "stream"
                }
            ], 
            "source": "# Crea la estructura para almacenar los valores de las variables\nfeat_values = { }\n\n# Obtiene el n\u00famero de valores distintos.\nfor var in range(n_feat):\n    ############\n    # COMPLETAR\n    feat_values[features[var]] = dataRDD.map(lambda x: x[var]).distinct().collect()\n\n# Extrae el n\u00famero de valores.    \n############\n# COMPLETAR\nnum_feat_values = dict(map(lambda x: (x[0],len(x[1])), feat_values.items()))\n\n# Los imprime\nprint(feat_values)\nprint(num_feat_values)\n\n#{'CARRETERA': ['Nacional', 'Autov\u00eda'], 'VELOCIDAD': ['Normal', 'Alta'], 'ALQUILER': ['Caro', 'Barato'], 'VEHICULO': ['Berlina', 'Deportivo']}\n#{'CARRETERA': 2, 'VELOCIDAD': 2, 'ALQUILER': 2, 'VEHICULO': 2}", 
            "execution_count": 16
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "#### Probabilidad marginal de la clase. \n\nEn esta parte, vamos a optener la probabilidad de la clase, $P(Y)$. Para cada valor de la clase $c_k \\in Y$ la probabilidad (por m\u00e1xima verosimilitud) se obtiene como:\n$$\np(Y=c_k) = \\frac{count(c_k)}{N}\n$$\n\ndonde $count(c_k)$ es el n\u00famero de instancias con la clase $c_k$, y $N$ el n\u00famero de instancias.\n\nEn primer lugar contaremos el n\u00famero de instancias de cada clase, y las almacenaremos en la variable `c_y`, que es un diccionario. En este proceso, hemos de tener en cuenta que los datos se encuentran en un RDD, y el tama\u00f1o de la base de datos, por lo que utilizaremos la funci\u00f3n de Spark `countByValue`."
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "outputs": [
                {
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "{'Berlina': 8, 'Deportivo': 8}"
                    }, 
                    "execution_count": 8
                }
            ], 
            "source": "############\n# COMPLETAR\nc_y = dict(dataRDD.map(lambda x: x[-1]).countByValue())\nc_y\n# {'Berlina': 8, 'Deportivo': 8}", 
            "execution_count": 8
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "Una vez hecho el recuento, se pueden calcular las probabilidades. En lugar de m\u00e1xima verosimilitud, utilizaremos la _correcci\u00f3n de Laplace_. Esto equivale (no nos escucha ning\u00fan estad\u00edstico) a a\u00f1adir un ejemplo de cada clase. Por tanto, la probabilidad de que una instancia sea de la clase $c_k$ ser\u00eda:\n\n$$\np(Y=c_k) = \\frac{count(Y=c_k)+1}{N+K}\n$$\n\ndonde $N$ es el n\u00famero de ejemplos y $K$ es el n\u00famero de clases. En este caso, puesto que ya se dispone de los par\u00e1metros necesarios, y no hay que procesar la base de datos, no se utilizar\u00e1n las primitivas de Spark."
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "outputs": [
                {
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "{'Berlina': 0.5, 'Deportivo': 0.5}"
                    }, 
                    "execution_count": 17
                }
            ], 
            "source": "############\n# COMPLETAR\np_y = {key:(value+1.0)/(n_inst+num_feat_values[class_feature]) for key,value in c_y.items()} \np_y\n#{'Berlina': 0.5, 'Deportivo': 0.5}", 
            "execution_count": 17
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "<br>\n\n---\n\n#### Probabilidades condicionales.\n\nA continuaci\u00f3n, se van a obtener la probabilidad condicional de cada variable $X_i$ dada la clase, es decir $P(X_i|Y)$. La probabilidad de que $X_i=x_l$ cuando $Y=c_k$ se obtiene, por m\u00e1xima verosimilitud, como:\n\n<br>\n$$\np(X_i=x_l|Y=c_k) = \\frac{p(X_i=x_l,Y=c_k)}{p(Y=c_k)}=\\frac{count(X_i=x_l,Y=c_k)}{count(Y=c_k)}\n$$\n\n<br>\n\nLa operaci\u00f3n $count(X_i=x_l,Y=c_k)$ se tiene que ejecutar para cada combinaci\u00f3n de valores de $X_i$ e $Y$, y para todas las variables $X_i$.  Con el fin de paralelizar esta operaci\u00f3n, vamos a transformar cada instancia en una lista de tuplas (una por variable) que contengan el nombre de la variable, el valor, y el valor de la clase.  Por ejemplo, la instancia\n\n    ['Barato', 'Normal', 'Nacional', 'Berlina']\n\n\nse transformar\u00eda en\n\n\n    [('ALQUILER', 'Barato', 'Berlina'), ('VELOCIDAD', 'Normal', 'Berlina'), ('CARRETERA', 'Nacional', 'Berlina')].\n\nDefinir una funci\u00f3n, denominada `expand`, que lleve a cabo esta transformaci\u00f3n."
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "outputs": [
                {
                    "text": "[('ALQUILER', 'Barato', 'Berlina'), ('VELOCIDAD', 'Normal', 'Berlina'), ('CARRETERA', 'Nacional', 'Berlina')]\n", 
                    "name": "stdout", 
                    "output_type": "stream"
                }
            ], 
            "source": "def expand(instance):\n    values = []\n    for nvar,value in enumerate(instance[:-1]):\n        values.append((features[nvar], value, instance[-1]))\n    return values\n\nprint(expand(['Barato', 'Normal', 'Nacional', 'Berlina']))\n#[('ALQUILER', 'Barato', 'Berlina'), ('VELOCIDAD', 'Normal', 'Berlina'), ('CARRETERA', 'Nacional', 'Berlina')]", 
            "execution_count": 10
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "Transformar las instancias mediante `map` y  generar una lista unidimensional de tuplas mediante `flatMap`. Almacenar el resultado en una variable denominada `data_expRDD`."
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "outputs": [
                {
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "[('ALQUILER', 'Barato', 'Berlina'),\n ('VELOCIDAD', 'Normal', 'Berlina'),\n ('CARRETERA', 'Nacional', 'Berlina')]"
                    }, 
                    "execution_count": 12
                }
            ], 
            "source": "############\n# COMPLETAR\ndata_expRDD = dataRDD.map(expand).flatMap(lambda x: x)\ndata_expRDD.collect()[:3]\n\n#[('ALQUILER', 'Barato', 'Berlina'),\n# ('VELOCIDAD', 'Normal', 'Berlina'),\n# ('CARRETERA', 'Nacional', 'Berlina')]", 
            "execution_count": 12
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "A partir de la base de datos anterior, obtenemos el n\u00famero de veces que aparece cada tupla ($X_i,x_l,c_k$), que corresponde a $count(X_i=x_l,Y=c_k)$, y lo almacenaremos en un RDD pareado denominado `pair_counts`. Para ello, hemos de usar `reduceByKey`. Almacenar el resultado posteriormente en un diccionario denominado `c_xy`.\n\n__Nota__: Se podr\u00eda proceder de manera distinta, pero as\u00ed vemos c\u00f3mo hacer el conteo con `reduceByKey`."
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "outputs": [
                {
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "{('ALQUILER', 'Barato', 'Berlina'): 6,\n ('ALQUILER', 'Barato', 'Deportivo'): 2,\n ('ALQUILER', 'Caro', 'Berlina'): 2,\n ('ALQUILER', 'Caro', 'Deportivo'): 6,\n ('CARRETERA', 'Autov\u00eda', 'Berlina'): 4,\n ('CARRETERA', 'Autov\u00eda', 'Deportivo'): 4,\n ('CARRETERA', 'Nacional', 'Berlina'): 4,\n ('CARRETERA', 'Nacional', 'Deportivo'): 4,\n ('VELOCIDAD', 'Alta', 'Berlina'): 4,\n ('VELOCIDAD', 'Alta', 'Deportivo'): 8,\n ('VELOCIDAD', 'Normal', 'Berlina'): 4}"
                    }, 
                    "execution_count": 13
                }
            ], 
            "source": "#c_xy = dict(data_exp.countByValue()) # No devuelve un RDD sino un diccionario directamente.\n\n############\n# COMPLETAR\npair_counts = data_expRDD.map(lambda x: (x,1)).reduceByKey(lambda v1,v2:v1+v2)\nc_xy = pair_counts.collectAsMap()\nc_xy\n\n#{('ALQUILER', 'Barato', 'Berlina'): 6,\n# ('ALQUILER', 'Barato', 'Deportivo'): 2,\n#...}", 
            "execution_count": 13
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "Ahora podemos calcular las probabilidades condicionales. Igual que en el caso anterior, aplicamos la correcci\u00f3n de Laplace:\n\n$$\np(X_i=x_l|Y=c_k) = \\frac{count(x_l,c_k)+1}{count(c_k)+L}\n$$\n\ndonde $L$ es el n\u00famero de valores que puede tomar la variable $X_i$. Es decir, equivale a a\u00f1adir un caso con cada etiqueta.\n\nObtener las probabilidades y almanacenarlas en un diccionario denominado `p_xy`. Para ello, utilizar la funci\u00f3n `map` sobre `pair_counts`."
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "############\n# COMPLETAR\n#p_xy = pair_counts.map(lambda count: (count))\np_xy.collectAsMap()\n\n#{('ALQUILER', 'Barato', 'Berlina'): 0.7,\n# ('ALQUILER', 'Barato', 'Deportivo'): 0.3,\n# ('ALQUILER', 'Caro', 'Berlina'): 0.3,\n#...}", 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "Una alternativa es hacerlo con Python normal, operando sobre el diccionario `c_xy`."
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "outputs": [
                {
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "{('ALQUILER', 'Barato', 'Berlina'): 0.7,\n ('ALQUILER', 'Barato', 'Deportivo'): 0.3,\n ('ALQUILER', 'Caro', 'Berlina'): 0.3,\n ('ALQUILER', 'Caro', 'Deportivo'): 0.7,\n ('CARRETERA', 'Autov\u00eda', 'Berlina'): 0.5,\n ('CARRETERA', 'Autov\u00eda', 'Deportivo'): 0.5,\n ('CARRETERA', 'Nacional', 'Berlina'): 0.5,\n ('CARRETERA', 'Nacional', 'Deportivo'): 0.5,\n ('VELOCIDAD', 'Alta', 'Berlina'): 0.5,\n ('VELOCIDAD', 'Alta', 'Deportivo'): 0.9,\n ('VELOCIDAD', 'Normal', 'Berlina'): 0.5}"
                    }, 
                    "execution_count": 18
                }
            ], 
            "source": "############\n# COMPLETAR\np_xy = {key:(value+1)/(c_y[key[2]]+num_feat_values[key[0]]) for key,value in c_xy.items()}\np_xy", 
            "execution_count": 18
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "---\n\n<br>\n### Naive Bayes: Clasificaci\u00f3n\n\n\nUna vez se dispone del modelo (tabla con las probabilidades marginales y condicionales), vamos a utilizarlo para clasificar nuevos casos. Por ejemplo:\n\n    ALQUILER = Barato, CARRETERA = Alta, VELOCIDAD = Nacional"
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "import numpy as np\ninstance = ['Barato', 'Alta', 'Nacional']", 
            "execution_count": 15
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "Dada una entrada, Naive Bayes predice la clase $y$ tal que:\n\n$$\nc_\\theta(x) = \\underset{y \\in \\{c_1, \\ldots, c_k\\}}{argmax} P(x_1,\\dots,x_n,y) \\, = \\underset{y \\in \\{c_1, \\ldots, c_k\\}}{argmax} \\,  \\prod_{i=1}^{n}  P(x_i|y) \\cdot P(y)\n$$\n\nPor tanto, hay que calcular, para esta instancia en concreto:\n\n$$\nP(Barato, Alta, Nacional, c_k)=P(Barato|c_k)\\cdot P(Alta|c_k) \\cdot P(Nacional|c_k) \\cdot P(c_k)\n$$\n\npara cada clase $c_k$, y devolver el valor de $c_k$ qu eproduzca un mayor valor.\n\nImplementar una funci\u00f3n denominada `calc_probs` que reciba como par\u00e1metro una instancia y devuelva la probabilidad calculada para cada clase.\n\n__Nota__: Esta funci\u00f3n no hace uso de Spark."
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "outputs": [
                {
                    "text": "{'Berlina': 0.087499999999999994, 'Deportivo': 0.067500000000000004}\n", 
                    "name": "stdout", 
                    "output_type": "stream"
                }
            ], 
            "source": "def calc_probs(instance):\n    probs = {}\n    # Para cada clase\n    for cl in feat_values[class_feature]:\n    ############\n    # COMPLETAR\n        probs[cl] = np.prod(list(map(lambda x: p_xy[(features[x[0]],x[1],cl)], \n                                 enumerate(instance))))*p_y[cl]\n    return probs\n\nprint(calc_probs(instance))\n#{'Berlina': 0.087499999999999994, 'Deportivo': 0.067500000000000004}", 
            "execution_count": 22
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "El caso anterior, vemos que se clasificar\u00eda como 'Berlina', ya que es la clase para la que la distribuci\u00f3n de probabilidad conjunta es mayor. Sin embargo, si queremos calcular la probabilidad asociada a cada etiqueta, hemos de normalizar. Es decir, dividir cada probabilidad por la suma de las probabilidades.\n\n$$\nP(y|\\mathbf{x}) = \\frac{\\prod_{i=0}^n P(x_i|y)\\cdot P(y)}{\\sum_{y}\\prod_{i=0}^n P(x_i|y)\\cdot P(y)}\n$$\n\nImplementar una funci\u00f3n denominada `classify`, que reciba una instancia y devuelva una tupla con la clase predicha y su probabilidad.\n\n__Nota__: Esta funci\u00f3n llama a la anterior, y tampoco hace uso de Spark."
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "outputs": [
                {
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "('Berlina', 0.56451612903225801)"
                    }, 
                    "execution_count": 28
                }
            ], 
            "source": "def classify(instance):\n    sum_probs = 0\n    probs = calc_probs(instance)\n    sum_probs = np.sum([prob for prob in probs.values()])\n    ############\n    # COMPLETAR\n    return max([(key,value/sum_probs) for key,value in probs.items()], key=lambda x:x[1])\n    \nclassify(instance)\n\n# ('Berlina', 0.56451612903225801)", 
            "execution_count": 28
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "Clasificar esta nueva instancia (falla)."
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "instance = ['Caro', 'Normal', 'Nacional']\n#print(classify(instance))", 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "La instancia anterior falla porque no existe la probabilidad asociada y, por tanto, no se pueden calcular las probabilildades asociadas a esa instancia, ya que `p_xy[('VELOCIDAD','Normal','Deportivo']` devuelve error. Para solventarlo, se puede reescribir la funci\u00f3n `calc_probs`, de modo que acceda a las probabilidades con el m\u00e9todo `get`, que permite devolver un valor por defecto en caso de error. \n\nEl valor es, al hacer correcci\u00f3n de laplace debe ser \n$$\np(X_i=x_l|Y=c_k) = \\frac{count(x_l,c_k)+1}{count(c_k)+L} =\\frac{1}{count(c_k)+L}\n$$\n\nYa que $count(x_l,c_k)=0$. Reescribir la funci\u00f3n `calc_probs`."
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "outputs": [
                {
                    "text": "('Berlina', 0.56451612903225801)\n('Berlina', 0.68181818181818188)\n", 
                    "name": "stdout", 
                    "output_type": "stream"
                }
            ], 
            "source": "# Por claridad, reescribimos la funci\u00f3n de consulta a p_xy\ndef prob(var,val,cl):\n    return p_xy.get((var,val,cl), 1.0/(c_y[cl]+num_feat_values[var]))\n\n# Reescribimos calc_probs\ndef calc_probs(instance):\n    probs = {}\n    # Para cada clase\n    for cl in feat_values[class_feature]:\n    ############\n    # COMPLETAR\n        probs[cl] = np.prod(list(map(lambda x: prob(features[x[0]],x[1],cl),\n                                     enumerate(instance))))*p_y[cl]\n    return probs\n\ninstance = ['Barato', 'Alta', 'Nacional']\nprint(classify(instance))\ninstance = ['Caro', 'Normal', 'Nacional']\nprint(classify(instance))\n", 
            "execution_count": 29
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "", 
            "execution_count": null
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5 (Experimental) with Spark 1.6", 
            "name": "python3", 
            "language": "python"
        }, 
        "anaconda-cloud": {}, 
        "language_info": {
            "version": "3.5.2", 
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }, 
            "file_extension": ".py", 
            "name": "python"
        }
    }, 
    "nbformat_minor": 2, 
    "nbformat": 4
}